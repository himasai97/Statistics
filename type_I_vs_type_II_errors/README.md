In statistical test theory, we often check if the result of a given test corresponds with the reality using hypothesis testing. This statistcal test requires an initial assumption, written as an unambiguous statement, called the null hypothesis (H<sub>0</sub>). It is the default hypothesis where the assumption is usually that there is no nothing new in a given scenario, like: "this new product is not efficient" or "this new website layout has less or same conversions as original". Here, a positive result corresponds to rejecting the null hypothesis (not have cancer, more conversions, efficient).

If the result of the test corresponds with reality, then a correct decision has been made (e.g., product is efficient and is tested as efficient, or the product is not efficient and is tested as not efficient). However, if the result of the test does not correspond with reality, then it is determined to be one of the two errors: **type I** and **type II** error.

### Type I Error (False Positive Error)

In this case, the null hypothesis is true but is falsely rejected. In simple terms, we can term it as a case of "false alarm".

### Type II Error (False Negative Error)

In this case, the test fails to reject the null hypothesis when it is actually false.

## Objectives
The aim of this project is to:
- differentiate how Type I and Type II errors relate to the p-value
- simulate and visualize scenarios involving Type I and Type II errors
- present the relationship between alpha and Type I/Type II errors.
